A new approach to knowledge distillation is proposed, where low-level features from a teacher network are used to guide the training of deeper features in a student network. This approach, called "Knowledge Review," has been shown to significantly improve the performance of the student network.

The Knowledge Review framework employs a residual learning mechanism to stabilize and enhance the learning process. It introduces an attention-based fusion (ABF) module and a hierarchical context loss (HCL) function to further improve the knowledge transfer. The ABF module helps integrate multi-level information effectively, while the HCL function guides the student network to focus on important features at different levels.

By applying this framework, the student network can leverage the knowledge gained at previous stages to enhance its understanding and context of the current feature being learned. This approach enables the student to improve its learning effectiveness and achieve better performance in various computer vision tasks. The paper demonstrates the advantages of the Knowledge Review strategy through extensive experiments, showcasing state-of-the-art results for compact models on multiple computer vision tasks.

The proposed method has been evaluated on a variety of computer vision tasks, including classification, object detection, and instance segmentation. The results show that the proposed method consistently outperforms previous knowledge distillation methods. The authors attribute this improvement to the enhanced supervision provided by the low-level teacher features, which guide the training of deeper student features. This new perspective on connection path design provides a valuable contribution to the field of knowledge distillation, offering a simple yet effective way to improve student network performance.
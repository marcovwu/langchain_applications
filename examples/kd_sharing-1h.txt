
 Speaker 0:

 Speaker 1: So I'm going to share a little bit about what's going to happen today, and I'm going to start by saying that if you're going to do knowledge distillation, it's going to be very intuitive. So the first piece is going to talk about the feature from the feature end of the survey to now counting as the current survey to the best one. So I'm not going to talk about this, I'm going to talk about this review, and I'm going to talk about this review, and I'm going to talk about this review, and I'm going to talk about this review. The concept of giclabel is that if you have a vector that's ready to have an output, then the embedding vector can do the learning, or the logic of each of these classes can do the learning, which is probably this column below. There are three of them, and I'm going to talk about two of them, DekalbKDD, which is DKD and NKD, and then these two papers are the most recent ones, and they're the most useful ones. First of all, let's look at the featurelabel on this side, and I'll read it again. This review is KD, and I'm going to call it RKD. B might be one of the features, and then take the teacher to teach the student, and then maybe a little bit crazier, maybe every feature wants to learn, and then it pulls like this. So the concept is, how do I extract information from each layer of the teacher to teach the student?

 Speaker0: Yes, this is RKD.

 Speaker 1: You can imagine that these features, which are actually very deep, like C, are not very sure what his meaning is, so it's not really that effective to learn.

 Speaker 0: You said you said which one more

 Speaker 1: It's like Stage 1 assuming that in a paradigm like this, you might be in Stage 1.

 Speaker 0: It's an oh

 Speaker 1: Right, right, right, that's because he's wrong, that's supposed to be a shallow right, that's supposed to be a shallow right, that's supposed to be a shallow right, that's supposed to be a shallow right.

 Speaker0: Conceptually, let's say there's a waiting for each of the lower layers, and then these waiting might be, let's say, the beginning, maybe it's 0.25 quarters, so let's say that's true or not necessarily true, but e Eventually it's going to feel like over time it's going to feel like this waiting is going to have some adjustments, like the training is going to be important, like the front four layers, and then the back EPOC is going to feel like it's going to be the last stage four. And then this is a kind of

 Speaker 1: So this is actually one of the ways that we're going to focus on these layers, and if we're going to go back and forth, then he's going to come up with this architecture, and this is the first one, and this is his architecture, and this is the way he's going to lead us to think. So the student-teacher, that's probably the first thing that he's going to say is that he wants to learn, and I want to go to the deepest part of this, and I want to learn this information from each teacher, and it's probably going to have to be aligned, so it's going to have to be pulled out, and it's going to have to do some transformations and so on. So each layer is going to do this, and then each layer is going to do this, and then each layer is going to do this, and then each layer is going to do this, and then each layer is going to do this, and then L2 is going to do this, and it's going to take a lot of time.

 Speaker 0: That

 Speaker 1: That's probably the third one that's going to be C. That's going to be a simplification.

 Speaker0: This gray represents some of the networks, some of the NN networks, the gray ones are the bugs, or that one is the embedding.

 Speaker 1: On this side, you can think about it like this, the concept is that you're a student, you're going to take this feature directly, and you're going to follow the teacher, and it's not going to be the same size, so it's probably going to be some very simple transitions. The conversion of a possible simple feature

 Speaker0: In gray, it's kind of like mapping the embedding or the T-shirtspace between the studio and the T-shirt.

 Speaker 1: On the gray is a map out and then have to be as big as a T-shirt to calculate L2 is the simplest concept is like that and then C is to merge you can go to each one to align can also design a merge block to become a simplify the need to calculate L2 is the number of times this is the concept proposed by C that this side is more clear to draw it here is proposed that this bot it defines it as ABF's bot will wait to introduce and then apply the concept separately it's kind of like the concept of residual is to go it's no longer every layer has to be entered But it's going to be able to look at it from the deepest layer, and the first layer is going to be able to look at it from the deepest layer, and the first layer is going to be able to look at it from the deepest layer, and the first layer is going to be able to look at it from the deepest layer. The upper layer of the student and then together in this bar to go out again to survive one and then push all the way back to get this gray layer feature like that and then get this layer he's on this side and not calculating L2 he didn't come up with a HCL calculation loss method and then come up with In the end, this so-called

 Speaker 0: RKD

 Speaker 1: This concept is like this.

 Speaker0: There's a feeling that it's like the last layer because it's actually the most important layer because the further back you go, the closer you get to RK.

 Speaker 1: D is right.

 Speaker0: It's kind of like taking this information from the last layer, like we're observing a person, maybe observing some of his important characteristics, mapping it to his previous layer, there's a kind of attentio.

 Speaker 1: It feels good.

 Speaker0: Like to say that we're now very focused on his glasses so how do we go from these layers above to find more and more glasses related to some of the future that might be because it's been lost instead of not being used at all on the top layer

 Speaker 1: The information is correct, so it's like you said, this block is a concept of attention, this block on the left of the APF, this concept is this side, this green one, this is the one that just came in from the deepest, the deepest. feature and then maybe do a little bit of upsample to make it as big as this layer and the layer above it and then maybe go to concretize it and then go to give it a conclusion to decide where you want to focus your attention on. So this is the APFblock, and this is the HCL, and this is the gray block that I'm going to get for each layer, and I'm going to have to calculate this loss. So this is very simple, and he's going to come up with a concept that's kind of like SPP, and I might have a different pulling for each layer, and he's going to go to a different scale, and he's going to do it separately.

 Speaker 0: L2

 Speaker 1: So to calculate the loss and get is that his HCLR is nothing is just that it's similar to the concept of HCP to calculate the loss so these two add up is the RKDE this paper suggests the method is about

 Speaker 0: This is

 Speaker 1:

 Speaker 0: That's it

 Speaker 1: Can I not advise you? I have to advise you. I feel like I have to advise you.

 Speaker 0: He's too big for you.

 Speaker 1: Because this inverse is this tea.

 Speaker0: I've been asking him if he has a ticket.

 Speaker 1: I don't think Memory has that label with the teacher I don't think it's very good is that I added this RKD won't be so student this this ticket price is going to add too many people because he's also three three okay can II use my own sweet you Let's call that three, three, that's it, so it's not too much more teacher, if you want to save this, go to teacher, maybe people can't save anything.

 Speaker 0: Yes

 Speaker 1: Well, that's where it counts losses. It just counts it. It's just one of its losses.

 Speaker0: The top four are the bottom four that can go to the bottom of the friend, so I'm a little bit confused to say that these four losses are the same as the last four on the page.

 Speaker 1: Loss is not not not not one of these blocks this HZL is one of these just every time you're going to do one of the SPP concepts he's going to do pooling here is equal to doing four pooling four different sized poolin

 Speaker 0: yes

 Speaker 1: And then the teacher and the student do the same thing, and then the teacher and the student do the same thing, and then the teacher and the student do the same thing.

 Speaker 0:ling

 Speaker 1: You can look at different scales at different scales.

 Speaker0: Let's say this is level 2, this is level 4, this is level 8.

 Speaker 1: That's right.

 Speaker 0: This is the fourth level, this is going to be 8 times 6 times 12, right?

 Speaker 1: Right, right.

 Speaker 0: So maybe until the last layer, until the last layer becomes level 8, that's 163 times.

 Speaker 1:64 it doesn't do much it doesn't change anything actually it's the same button in every layer I look at it the number is 421 do three patches and that's it

 Speaker 0: like

 Speaker 1: I was just saying.

 Speaker0: I want to say that the graph isn't that big. So if the last layer today is actually already, let's say, that layer is 16th, then you have to turn it into a half, a quarter, an eighth.

 Speaker 1: It is possible.

 Speaker 0: That's a good third. I mean, let's say you have this original.

 Speaker 1: It's small enough.

 Speaker0: So if you're doing MVC at a deeper level, you're going to put that

 Speaker 1: It should be small.

 Speaker0: You can do that too, but it's probably already a big pixel.

 Speaker 1: It should be said that even if the feeling is really small, there is no loss in the end.

 Speaker0: Like now 128 times 64 this should be HW that's HW assuming 128 times 64 to some layer it's become like 64 this dimension has become an octave it's also become an eight that's the last layer of it is 8420

 Speaker 1: against 842 against against

 Speaker0: If there's a layer that's actually 0.4, that's going to be 4212 pixels.

 Speaker 1: Right, but it doesn't really affect me, I think it's okay, even if you don't have the money, it doesn't affect you as long as

 Speaker 0: A few more of this stuff, and you're right.

 Speaker 1: Right.

 Speaker 0: Yes

 Speaker 1: Okay, so this side is cutting directly from his comparison form, and this side is cutting directly from his comparison form, and this side is cutting directly from his comparison form, and this side is cutting directly from his comparison form, because I didn't look at the paper above.

 Speaker0: Singlelayer is the third but different me

 Speaker 1: I didn't go to see I can if I'm curious I can now I can now open to see should be different he can't reference 22 oh he's relection ah relectionknowledgedissolution That's right, that's right, that's right, that's right, that's right, that's right, that's right, that's right. GeNet, I'll be the first.

 Speaker0: Ask one I ask one question he this Walk may not be Uden is EMM, Geacher is Transformer Tra

 Speaker 1: In the nformer, but in the Transformer, I don't really know what the architecture of the Transformer is, what the output of each layer is going to be. Ansformer

 Speaker 0: Come and learn, that's all right, that's all right, that's all right.

 Speaker 1: That's how to teach MarvelNet with SwingTransformer, so as long as the dimensions are the same.

 Speaker0: Four is doing that voting? Similar to that kind of is four thirty-two going together how to go to my question comparatively like the main this city

 Speaker 1: That's it. That's it. That's it. That's it.

 Speaker0: Don't turn me around right now, I think it's like this right now. You can wait a little bit. The magic is that his W1 is 45 is 75.6 but he trained to be 77.1.

 Speaker 1: This is actually something that you see from time to time, so it's been discussed before that it would be really better, but it's a two-way street.

 Speaker0: And it's interesting to say that actually we're looking at ShuffleNetV1 and V2 ShuffleNetV1 and V2 are actually learning, although v2 is more accurate than v1 originally, but it's learning about the same, and it's actually ResNet3. It's like a teacher, although it's about the same resolution as the original teacher, but the ResNet32 program that it teaches the students learns especially well, ResNet50 teaches it out, it's only 69.89 pairs, so I think that might be the key. scriptfromalarge that's actually what I should use I'm just an example just say swing fist and median's four will make it stronger

 Speaker 1: The last time I did this experiment, I was just listening to you tell me why I shouldn't use the big one.

 Speaker 0: I mean it looks like a big one out of four right now.

 Speaker 1: Less than four out of one

 Speaker0: So one ResNet50 as a teacher is actually less than four ResNet32 is going to be two cards. So we can't compare it to each other.

 Speaker 1: It's his skill, his skill in putting data.

 Speaker0: It's hard to compare this to ResNet, but I'd like to see ResNet50 and ResNe.

 Speaker 1: T is definitely not the answer you want he's not going to put it like that that's the technique of writing paper otherwise he's going to put it all the same well everyone can go to know he must not be align this thing you're talking about why

 Speaker0: His ablation wasn't very complete. I thought he was shut out of both.

 Speaker 1: downCVPR2021

 Speaker0: I don't see him as the same student or the same teacher.

 Speaker 1: So that's right, there's no way he's going to put it like that, there's definitely a reason his paper is like that, and then there's two other papers that have experiments that are useful, like DKD and NKD. Generally calculated

 Speaker0:s in the

 Speaker 1: What's the category? Blue is the target, maybe green is the nontarget, what's the category? The law ensures that the teacher and the student have the same confidence in the target.

 Speaker0: What does arget mean?

 Speaker 1: That's your object, your category, you.

 Speaker0: What kind of goals?

 Speaker 1: Which category are you in?

 Speaker 0: You should always raise one when you come out.

 Speaker 1: That

 Speaker0: One of my questions is, let's say my teacher today is coco80 class, right, let's say I pick up ImageNet, let's say I'm ImageNet 1000 class today, then my target must be something that should be in that 1000 class. What kind of a subset is this?

 Speaker 1: Isn't its target your label your image's label just like that I'm the tiger that's the blue that's the tiger that's the tiger that's the tiger that's the rest of you you say 1,000 categories maybe 99 Nine categories are non-target, and this tiger category is target.

 Speaker0:t Suppose I should say so your this target and non-target is there in this hypothesis I'm all going to distill is this model of the 1001 class of the 1000 class that it also refers to the physics that appears in this picture

 Speaker 1: Right about this image right it's c

 Speaker 0:l

 Speaker 1: This is an assification.

 Speaker0: Image pair should be only one such pair should say

 Speaker 1: Right, there's only one situation that doesn't have to be complicated to explain. Right, so right, so that's it. So as long as there's this difference, you can imagine that we're going to have, let's say, 1,000 classes, and we're going to subtract, and we're going to have to do something like softmax or whatever to make it add up to 1. So let's say you're going to subtract this one target from the probability that you're going to be left with a sum that's not going to be equal to one. Let him learn that way.

 Speaker 0: of

 Speaker 1: You're going to let him learn what's called the probability that it doesn't make sense in mathematics that two fundamentals can't add up to the same thing.

 Speaker 0: Because

 Speaker 1: For the next one, it's a similar question, but I think it's a little bit simpler.

 Speaker 0: Let me help you explain this way. A lot of A's and B's are big.

 Speaker 1: How bad is the performance?

 Speaker 0: So you're going to use the OK form directly, because I think these concepts sound like they've been experimented with.

 Speaker 1: Yes, there's nothing wrong with that.

 I don't think there's anything to see.

 Speaker 1: That's a bit of a mistake.

 Speaker 0: You can play it later.

 Speaker 1: It's definitely going to be useful to run out first anyway.

 Speaker 0: This is ta.

 Speaker 1: rgetN is non-target just took it apart so you can see his own experiment

 Speaker0: So it has to be non-target

 Speaker 1: His data is indeed non-target and then contributes better

 Speaker 0: You are right.

 Speaker 1: Right, right, right, right.

 Speaker0: He says you're not a non-targe

 Speaker 1: It's all about him suddenly discovering

 Speaker0: I think the concept of it is kind of like you can go back to this one it has a formula it all feels like it's these two distributions it's supposed to be two tailroad it's information it's usually going to feel particularly high for a cla I think it means that it's actually higher than this. It's actually 0.7 overall. It's 0.5 overall. So actually you spend a lot of time actually picking this 0.5 and this other game 0.7 and the other game but there's no other because I basically I just want this target to be high so I use 0.7 and 0.5 it doesn't matter it just targets a cl Log of ass of course it wants 0.7 and 0.7 this side is 0.7 this side is 0.7 but just these two count well then the rest of him actually doesn't take this 0.5 and this other distance 0.7 or think about that side he just thinks about other classes like let's say today this is tiger then he might have Some, for example, let's say there's a type of cat that has characteristics like this tiger, and his response is going to be higher, and there's a type of car, and there's a type of car that doesn't have a car, and it's going to be lower. I feel like he's a non-target.

 Speaker 1: I'm actually thinking about him, and what I'm really looking at on his side is not being affected by the target, like the hypothesis that you just gave, the hypothesis that the teacher might be 0.7 tiger on this one, maybe 0.99 on that one, maybe 0.99 on that one. The teacher is very confident that you're going to think that the remaining non-target value is 0.01 plus the score.

 Speaker0: What you want to learn is the non-target coherence.

 Speaker 1: Right, but it's the same as not working at all, so he can dismantle it like this, and for the non-target, I can go and give him some weight, and I can give him some more contribution to this learning. So it's going to be on this side, and it's going to be on this side, and it's going to be on this side, and it's going to be on this side, and it's going to be on this side.

 Speaker0: A little bit of logic is not logic.

 Speaker 1: No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no. So it's a very simple way to learn to go beyond these featurebased things, but I think it's you running out of these results that I'm saying these words. This is the last one, this is the DKD, this is the NKD, this is the Normalize, so I just talked about this problem, and I just said that the probability that the non-target is going to be separated is going to be different. That's it, that's it, that's it, that's it, that's it, that's it, that's it, that's it, that's it.

 Speaker 0: I feel a lot like the last one.

 Speaker 1: That's a lot.

 Speaker 0: Normal

 Speaker 1: ze dismantled the pair and then but the more the more the normalize this action so he did it again

 Speaker0: Does he have an aside DKD?

 Speaker 1: There's definitely one for him because he's more like DKD than he's more like DKD like that but that's not his main he's mainly mentioned one but he's got another one called USKD all generic called NKD in this one not generic NKD I was wrong all in this one this is its One of the concepts is that it's very simple, and then the other one is that it's a little bit more complicated, and the concept is that he wants to learn on his own, so he has to generate a so. So the source of this generate is going to be through this s, which is the student himself, and then the first thing he's going to do is he's going to square the value of this prediction. So let's say that in the early days of learning, it's possible that each category has a similar probability, and then he can square it, and he can expand some of the differences between them, and he can take what I think today is the category that he gave, and he can take it, and he can give it to someone else.

 Speaker 0: The big difference

 Speaker 1: Because the first step is to generate a new SoftTargetLabel.

 Speaker 0: a category

 Speaker 1: So he's going to focus on this student, so he's going to generate the corresponding softlabel, so he's going to square it, he's going to pull the big difference, he's going to try to figure out the difference between them, and then he's going to add this VT, this VT source. This is the value of the teacher, this is the value of the teacher, this is the value of the target, this is the value of the target, this is the value of the target, this is the value of the target. I'm going to go over this is this is this is this is this is this is this is this is this is this is this is this is this is this is this is this is this is this is this And then you can take this PT, and it's as simple as this EntropyLoss and the original Student, and this is what it generates in this Target, and it does this. And then the second thing, this WeakSupervision, it's also going to take our last output, the feature before the logic, and it's going to do it again. So let's do it again, and then we'll do it again, and then we'll do it again, and then we'll do it again.

 Speaker 0: and

 Speaker 1: In addition to hardlevel, maybe I'll learn how to match myself to the teacher, and then the non-target will be the next weak soup. I don't know if you've heard of a probability distribution called ZIPF. Although they're not in this category, but the teacher told me about the teacher, maybe I'll give him an example.

 Speaker 0: 0.8% remaining

 Speaker 1: What's 0.2% 0.1% 0.0% that might be 0.2% 0.1% 0.0% that might be 0.2% and 0.1% that might be what Kurt said that there's a cat in there that might be more important so he's trying to use that concept here to make a I'm going to make those zero points even if he's just zero points two I'm going to make those his contributions come up high not everyone agrees to learn so based on that concept he's going to go he's going to take out this WeekSupervision first The probability of this coming out is the probability of the original student coming out.

 Speaker 0: A fusion

 Speaker 1: So this merger could be in his paper maybe it's WeekSupervision came out to him 90% and then the original Student accounted for 10% and then about this ratio to merge into a new probability value that this new probability value it would no longer be Let's call it a probability value. Let's call it a ranking. Let's call it a ranking. But what it really means is that it's going to be able to go and it's going to be able to go and it's going to be able to go and it's going to be able to go and it's going to be able to go and it's going to be able to go and it's going to be able to go So the probability of having a relatively large weight is going to be this NofZ in the front, and this N is going to be Normalize, and that's just going to be Normalize for each of them, and then calculate it.

 Speaker0: From the above formula, ST is a logic pair for the student based on the target, and then PT is a superbite logic because this superbite logicP should have been

 Speaker 1: VT

 Speaker0: Just say that in addition to BT he took into account the situation of ST to solve the pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair

 Speaker 1: Right, right, right

 Speaker0: to to to to to to to to to to to to to to to to to to to to to to to to to to to to

 Speaker 1: Right, right, right

 Speaker0: pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair pair

 Speaker 1: Right, right, right, right, right, right.

 Speaker0: On the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand.

 Speaker 1: It's supposed to be undercover, but he's additionally creating another head against another head to do this WeakSuperTeacher so

 Speaker0: This is only for training.

 Speaker 1: It's time for training.

 Speaker0: Okay, so there's a WI, and this WI is the student who comes in, and his WI is in the Earth's gravity, and he puts the WI and the VI through a plus a total loss, a superteacher named Weak.

 Speaker 1: This VI is the Teacher.

 Speaker0: I mean, but so WI is equal to saying that this side of WI feels like a logical node, right?

 Speaker 1: There is no mistake.

 Speaker 0: It's like going straight to the polls.

 Speaker 1: So it's going to have

 Speaker0: For 7 is to say that it does a more normalized supervision for 8 and 9 is to say that it becomes the whole

 Speaker 1: It's all right here.

 Speaker0: Eighty-nine doesn't matter he's equal to say eighth ninth or do one this is basically he's not logic he's equal to walk like a fea

 Speaker 1: Ture is totally f

 Speaker0: eature actually he pulled another head

 Speaker 1: But he's counting on Logic to go to Logic's place.

 Speaker 0:fea

 Speaker 1: You should be more rigorous, can't you say that?

 Speaker0: I'm judging other students today I'm doing an N this N is Normalization and then I'm going to go with Z this Z should also be from Pitcher? rict but

 Speaker 1: It's from WI.

 Speaker 0: So WI is 90 percent, SI is 10 percent, Z is 10 percent, and that's how it all came together.

 Speaker 1: Component means type loss you want to learn teacher but W is also S from teacher

 Speaker0: I'm also made of tea

 Speaker 1: This is Cher.

 Speaker0: It doesn't make much sense because the 12th, let's say the 7th, you can clearly see that the PT has a T-shirt ion in it, and then the 9th is a VI, which is a T-shirt ion.

 Speaker 1: No, he's probably expecting the ninth to learn that this WI will have T-shirt information.

 Speaker 0: OK is diffusion.

 Speaker 1: This should be the concept.

 Speaker0: Theoretically, this loss of a few inches could change over time.

 Speaker 1: Right, right, right, it is.

 So it's expected that the Ninth L-Week will probably decline by the end of last year and then the L-Nav will decline.

 Speaker 1: But this one, I'm going to wait and see if it's set up like this, but it does make a lot of sense, because it does.

 Speaker0: So if it's like you just said, I'm going to have to train well first, and if I'm going to be very weak, of course, I'm going to have to study a lot.

 Speaker 1: Yeah, maybe if it's the same, it should be longer.

 Speaker 0: That's it

 Speaker 1: But it's still going to go in this direction after all you've learned eight nine you're going to learn twelve nine at the same time right there's another comparison table over there you can actually see he has DKD and RKD I'm going to make a comparison that pair is going to make a comparison to me but this RKD because I just didn't mention that I have to see if it's the same RKD or not. It's the old RKD.

 Speaker0: NKD this but your experiment is not RKD will not review KD effect on me

 Speaker 1: Why didn't I do such a complete experiment I don't dare to say but it did raise quite a lot I didn't ablationstudy everything because I was for me

 Speaker 0: See also

 Speaker 1: Hold on a second.

 Speaker 0: Yes

 Speaker 1: Look at me, look at me.

 Speaker 0: Wait a second.

 Speaker 1: There is no RKD.

 Speaker 0: Yes

 Speaker 1: I'm adding a lot of separate words, but DKD is almost the same as NKD.

 Speaker0: So yeah, it's actually better to include both DKD and NKD.

 Speaker 1: It's 4 but it's cool you see I'm going from this 12 to 12.4 here is just one more RKD you can imp

 Speaker0:rove so much from 12 to I don't use the mouse directly

 Speaker 1: No mouse from these two 12 to 12.4 just one more RK

 Speaker 0: D is just zero more points.

 Speaker 1: Plus zero points is you look at the other you look at me combined DKD plus NKD plus JSKD is not much more than the previous one

 Speaker0: It would be more interesting to say that if you don't remember RKD plus DKD I'm asking a question this AQRSE is whether you have a validation test or you only have a validation

 Speaker 1: That's what testing is in marketing.

 Speaker0: I mean that dataset it has a publictestingset for you to report on this table?

 Speaker 1: It has public testing.

 Speaker0:et is that when you were investigating you had a validation set assigned to you

 Speaker 1: Yes, yes, yes, there is validation in training.

 Speaker0: But now let's say if there's no validation and no testing then I'm going to choose the best word based on validation because I might not be the 3rd or the VC and then the V in the V but Marco says he didn't do that so this is very Fair results, right, or you actually have these validations.

 Speaker 1: It's not validation, it's testing, but you're saying it's fair. I didn't look closely.

 Speaker 0: Separated This is not

 Speaker 1: This is valid.

 Speaker0:ationset This is te

 Speaker 1: Sting

 Speaker0: But it's still obvious because you look at your RKD plus BKDMKD or whatever but it's not clear if it's RKD if I just add BKD or just add NKD

 Speaker 1: I'm not sure because of this.

 Speaker0: Right, right, right, because RKD is based on C, right, so maybe RKD plus one, because you're going from NKD down, these are all actually doing things in logic.

 Speaker 1: The rest is logic. It's really fun. It's a lot of fun.

 Speaker0: Actually, to be honest, according to what you just said, adding NKD is actually adding DKD. Theoretically, according to what you just said, NKD is the same as saying that DKD has done homologization.

 Speaker 1: There's not much difference.

 Speaker0: There's no other way he's going to be able to do multi-round testing to make sure that this plus is really a fake force because you're only going to run it once.

 Speaker 1: Sure, I'm not doing research, it's okay, he's diluting those, so in general, if you want to publish a paper, you have to do three, three, five experiments.

 Speaker0: Your last experiment was how many times I first translated you in genetics

 Speaker 1: Have you done the same experiment three times?

 Speaker0: No, you have an average and a standard.

 Speaker 1: The difference between the two.

 Speaker0: There's going to be a lot of variation because I've trained a lot of times, a lot of times, a lot of times, a lot of times, a lot of times, a lot of times, a lot of times.

 Speaker 1: I'm tired of the follow-up and it's nothing to keep it going.

 Speaker 0: It sucks.

 Speaker 1: I was supposed to be 500 in the trend, and then I let him be 500 in the trend, and then it didn't change.

 Speaker0: Because you're the one that's also bindto I mean initially you're the one that's because you're the one that might have reached a minimum pair estimate is you're the one that started because if you're direct it's 1,000 instead of your 500 direct hit 1,000 then it might be Different

 Speaker 1: But if I re-chain it, it's going to be a little bit more frequent, so it's going to be a little bit better to jump out, but if I re-chain it, it's not going to change.

 Speaker0: This kind of basically feels like RKD is for Future and then DKDMKD is for LogicUSKD is for both Logic and Futu

 Speaker 1:re is correct, but it can also be strictly counted as Logi.

 Speaker 0:c and so on.

 Speaker 1: It doesn't count.

 Speaker0: I'm curious about one thing, is there anyone who does HardLabelHardLabel? What does it mean that when you go straight out, you just pull it into a zero and then you train it as a HardLabel.

 Speaker 1: That's the Sudo La.

 Speaker0:belSudoLabel is divided into two types called SoftLabe

 Speaker 1: Yes, yes, yes, yes, I said you did.

 Speaker0: with HardLabel

 Speaker 1: But if HardLabel should generally not do this experiment, then if you only have that when HardLabel is better.

 Speaker0: No, I'm just saying that I'm curious to see what your performance would be like if you did it directly with HardLabel, because if you did it with HardLabel, you could actually emphasize it as your pre-generated Lab. And then I'm going to do the full name, and I'm just curious to see if it's going to improve some of the emphasis on DB. rdLabelLogic must be 8% and then we know from HardLabel to SoftLabel

 Speaker 1: Well, I think I think it's a waste of time to say that this can definitely be done, but you're saying that it's the same as not believing that these people's paper is valid. I can't even get them to verify anything on such a large dataset.

 Speaker0: It's supposed to be precise, which means it tells us what percentage of the softlevel is going to be higher than his level, which means we're not sure.

 Speaker 1: Yeah, it's just to see if if I'm going to do this experiment today, I'm going to do it on the carpedataset, or this is not PowerData, I'm going to do it on other projects, this is RealID, this is DataCenter, and we're going to do it ourselves. If you want to do this, of course you have to be a real Consultant.

 Speaker0:on I see this thing should be talking off-level because now KD is on what le

 Speaker 1: Very well.

 Speaker0: It's supposed to be off-level, but I think if there's a paper, it's going to compare in that kind of paper, like it's comparing with ImageMesh, it's going to be more cross-class, right?

 Speaker 1: Yes, of course.

 Speaker0: I'm going to stand up later and I'm not going to point out that I'm curious if this fancy method is going to work without any complications. The 3% limit is right. HardLevel is actually

 Speaker 1: I don't know what I mean.

 Speaker0: I don't think it's going to be less than 3% even if we change the HardLevel.

 Speaker 1: I don't think it's going to go up. I guess it's not going to go up.

 Speaker0: It's actually because this DAS also has this DAS with a beam.

 Speaker 1: There's ah there's ah 9.5 that beam circle that comes out is the real H.

 Speaker 0:ardLeve

 Speaker 1: So I think it's going to be worse. I think it's going to be 8.8%.

 Speaker 0: This is Har.

 Speaker 1: The dLevel circle is the real target, right?

 Speaker0: Ah, because, for example, when you're really using Practical, your data might just not want to label you, and then you're going to take a big dock and invent these labels, these labels that you don't use directly. It's not conceptual, it's not conceptual, it's not conceptual, it's not conceptual, it's not conceptual.

 Speaker 1: Theoretically more accurate HardLabe

 Speaker 0: I agree

 Speaker 1: So it's supposed to be a test of how well it's going to perform if I don't want to label it today.

 Speaker0: I mean a noisy hardlabel might be better than a light beam hardlabel I always get that impression because I remember saying if today because actually the light beam might be the wrong standard or those things are hard to learn so I remember I saw some wor It's possible that there's going to be some misjudgments, but it's going to be easier for those who learn that behavior to get more accurate for smaller models, because smaller models would take a lot of time to try to make up for those mistakes. It's that if even the big models make mistakes, then the small models can actually learn better from the more uniform ones.

 Speaker 1: But it should be a little bit smaller, because if you magnify the scale, that impact might not be that big, and if today's Diase is big enough, that might happen to us, if it's commonly used, it's smaller. It's possible to say that, but I think Diase should be a big one.

 Speaker0: So this Compatible is kind of like that PID, and then you have to train the VITAdapter, or it's your big model, right, and then you train it, and then one of the ways is, of course, now there's some levelpick, but actually, even if you do levelpick it's still going to have some errors it's still going to have some noisy parts so if we're going to do an experiment like this it's going to be compared to these pictures you've done levelpick is there doing a human definition so it might be a little bit like The meaning of the people is that for this cost it's too hasty to take a big blind spot to mark whether to do it to talk to people it's possible to do a lot of people he promoted it's possible to do it badly

 Speaker 1: Very well.

 Speaker 0: I see.

 Speaker 1: Finally, I'm going to talk a little bit about this review of the resume. Okay, that's probably a lot of losses. All you have to do is write a config, and that's fine, and that's the config, and that's the config, and that's the config, and that's the config, and that's the config, and that's the config. Don't write it in this type, it's going to come in, it's going to be at the end of the loss, and you know what I'm going to use when I calculate this KDLoss. And then you can define the amount of data, and so on, and then you can declare it, and then you get this kdlossfunction, and you can use it directly here, and you can input the teacher and the student. And then, of course, the teacher and the student, because you're going to have different losses, you're going to have different features, you're going to have different logics, you're going to have different sizes. These are just examples, and it's actually very simple to use, just wrap it in a DigitalName and input it into this Logs, and it'll output the corresponding

 Speaker0: The corresponding logs you said FutureSi

 Speaker 1:ze to FutureN

 Speaker 0:ame

 Speaker 1: OK, it depends on, for example, if you're saying RKD, this part of RKD, this part of RKD is actually quite complicated, because this part of RKD, I think, if you really want to extract RKD, you have to first figure out which layer of your teacher you're going to extract from.

 Speaker0: This is more than this is more than Teacher and Due one is 384 fans

 Speaker 1: Right here.

 Speaker 0: That is

 Speaker 1: What?

 Speaker0: So we're going to do a one-by-one like you just showed me that review KD they're going to do a one-by-one I can one-by-one degrade

 Speaker 1: You can too.

 Speaker 0: There you go.

 Speaker 1: That's fine, that's fine, but I'm just like my MarvelNet and SwingTransformer, I pulled out the same position, I pulled out the same skill, WA, you're not dating, so you can pull in like this. Oyl C

 Speaker0:onnect right, but what about this PolyConnect?

 Speaker 1: If it's just that RKD actually has the width and the height reserved,

 Speaker 0: What is 16 percent?

 Speaker 1:8 You're saying in this D

 Speaker0: The process of instillation is done in the previous review KD if the number of layers is different.

 Speaker 1: When he

 Speaker0: It's going to do this thing OK that's why I just asked about this onebyone whether or not to persuade I think that even if I don't persuade I don't know he's just trying to balance the two sides

 Speaker 1: Is he got effective conversion if not confirmed his conversion will become very random will become should say something like this

 Speaker0: He's going to go to the back and look at the characteristics of the network, but I think this confirmation layer one is going to be bad for the monitor because you're looking at this layer one is going to be bad for me and then I don't know if there's going to be anything he's going to hold. The r

 Speaker 1: Fee here is also possible actually maybe it's interesting maybe maybe Randon down he's also right about Ra

 Speaker0:ndon under

 Speaker 1: Let's let Fei Feng go to school because Randon says we're here.

 Speaker 0: War he thinks the standard is about beauty

 Speaker 1: This is it.

 Speaker0: I feel that his kind of ED show or Hanson's kind of NAS technique was used a lot at the time, and I don't think it should be advised.

 Speaker 1: Well, anyway, it's time for Detection, and I think it's okay to say that RKD should be separable from some of the other classes, and there's a lot of classes that I think can be applied directly, like the DKD and NKD that I just mentioned. At least DKD  ⁇  NKD is super simple RKD may need to go to the most matrix may be able to enjoy when you come back to me to discuss I can be very simple help assist so you just

 Speaker 0: What you just said you did was use me.

 Speaker 1: Yes, all three of the ones I just mentioned.

 Speaker 0: use

 Speaker 1: All of it.

 Speaker 0: And then you say it's just like that.

 Speaker 1: It's not just that you're going to have to layer by layer when you sample I'm going to grab it when the sample plays the same scale the one that grabs it out the remaining feature is different actually the convolution is solved so fe Ature is different, it's very simple, but the point is that if I have a certain layer, I'm going to go from the original 1926-4 well, when I sample to 168, I'm going to go, I'm going to catch the student who has a 16x8 feature and who has a 16x8 feature. So as long as you can capture that scale as big as you can do RKD so you don't care what architecture you have to do this when sample ah sample etc. so it should be basically whatever architecture should have a chance to capture ah Unless you're flat at the beginning and then you lose it, and then you lose it, and then you lose it, and then you lose it, and then you lose it, and then you lose it.

 Speaker0: It's not that you can't get rid of pure BIDs, it's that they're flat.

 Speaker 1: If possible, you should see it.

 Speaker 0: You have to prepare your position.

 Speaker 1: You'll have to see.

 Speaker 0: Yes

 Speaker 1: But you have to be careful about RKD.

 Speaker0: But let's say your Fusion template is the same as Win on your Fusion hardware.

 Speaker 1: I'm not the same.

 Speaker 0: It's forced to make the teacher bigger anyway.

 Speaker 1: Forcing him to have the same ratio is fine. For example, if I say mine, my students are now 19264, I'll tell you to go to 12864. So one of the weeks I had a report was because the one I had from 1926-4 that was originally my teacher was 3 to 13 to 1 so RID wouldn't be a problem I wanted 1286 also out of a RID when I found out my teacher was going to repeat so I spent a week with my teacher on this loop, a two-to-one size, but as long as the ratio is the same, it doesn't matter how much you put in, how much you put in, how much you put in.

 Speaker0: Doris and Quiet Rain are online

 Speaker 1: No thank you.

 Speaker0: No, thank you very much. Econic can come first.